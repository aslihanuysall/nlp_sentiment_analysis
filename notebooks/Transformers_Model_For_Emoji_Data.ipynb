{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7debbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10166edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79eb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30399341",
   "metadata": {},
   "source": [
    "### Get Emoji Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f65df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_negative_data_list=[]\n",
    "raw_positive_data_list=[]\n",
    "raw_neutral_data_list=[]\n",
    "\n",
    "for negative_data_path in os.listdir(f\"./emoji_sentiment_data/raw_data/text_negative\"):\n",
    "    with open (f\"../emoji_sentiment_data/raw_data/text_negative/{negative_data_path}\", \"r\") as myfile:\n",
    "        temp_list=myfile.read().splitlines()\n",
    "        raw_negative_data_list.extend(temp_list)\n",
    "    \n",
    "for positive_data_path in os.listdir(\"./emoji_sentiment_data/raw_data/text_positive\"):\n",
    "    with open (f\"../emoji_sentiment_data/raw_data/text_positive/{positive_data_path}\", \"r\") as myfile:\n",
    "        temp_list=myfile.read().splitlines()\n",
    "        raw_positive_data_list.extend(temp_list)\n",
    "\n",
    "for neutral_data_path in os.listdir(\"./emoji_sentiment_data/raw_data/text_neutral\"):\n",
    "    with open (f\"../emoji_sentiment_data/raw_data/text_neutral/{neutral_data_path}\", \"r\") as myfile:\n",
    "        temp_list=myfile.read().splitlines()\n",
    "        raw_neutral_data_list.extend(temp_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a250748",
   "metadata": {},
   "source": [
    "### Get Emoji Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "403fc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_negative_data_list=[]\n",
    "processed_positive_data_list=[]\n",
    "processed_neutral_data_list=[]\n",
    "\n",
    "with open (f\"../emoji_sentiment_data/processed_data/negative/negative.txt\", \"r\") as myfile:\n",
    "    temp_list=myfile.read().splitlines()\n",
    "    processed_negative_data_list.extend(temp_list)\n",
    "    \n",
    "with open (f\"../emoji_sentiment_data/processed_data/positive/positive.txt\", \"r\") as myfile:\n",
    "    temp_list=myfile.read().splitlines()\n",
    "    processed_positive_data_list.extend(temp_list)\n",
    "\n",
    "with open (f\"../emoji_sentiment_data/processed_data/neutral/neutral.txt\", \"r\") as myfile:\n",
    "    temp_list=myfile.read().splitlines()\n",
    "    processed_neutral_data_list.extend(temp_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a11bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_negative_data_w_labels_list = [{\"text\":val,\"label\":\"negative\"} for val in processed_negative_data_list]\n",
    "processed_positive_data_w_labels_list = [{\"text\":val,\"label\":\"positive\"} for val in processed_positive_data_list]\n",
    "processed_neutral_data_w_labels_list = [{\"text\":val,\"label\":\"neutral\"} for val in processed_neutral_data_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00d27f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=processed_negative_data_w_labels_list + processed_positive_data_w_labels_list \n",
    "random.shuffle(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ca5c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=10, shuffle= True)\n",
    "\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=5, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08fd07b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100746"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3acd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/2_classes/train.json', 'w') as outfile:\n",
    "    json.dump(X_train, outfile)\n",
    "    \n",
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/2_classes/test.json', 'w') as outfile:\n",
    "    json.dump(X_test, outfile)  \n",
    "\n",
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/2_classes/validation.json', 'w') as outfile:\n",
    "    json.dump(X_val, outfile)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "282010fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/3_classes/train.json', 'w') as outfile:\n",
    "    json.dump(X_train, outfile)\n",
    "    \n",
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/3_classes/test.json', 'w') as outfile:\n",
    "    json.dump(X_test, outfile)  \n",
    "\n",
    "with open('/Users/aslihanuysal/Desktop/nlp_sentiment_analysis/emoji_sentiment_data/splitted_processed_data/3_classes/validation.json', 'w') as outfile:\n",
    "    json.dump(X_val, outfile)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35d53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"savasy/bert-base-turkish-sentiment-cased\"\n",
    "do_train=True\n",
    "#do_eval=True\n",
    "#do_predict=True\n",
    "per_device_train_batch_size=4\n",
    "per_device_eval_batch_size=4\n",
    "gradient_accumulation_steps=4\n",
    "num_train_epochs=4\n",
    "max_seq_length=512\n",
    "#load_best_model_at_end=\n",
    "#evaluation_strategy epoch=\n",
    "#save_strategy epoch=\n",
    "metric_for_best_model=\"loss\"\n",
    "overwrite_output_dir=True\n",
    "seed=1\n",
    "cache_dir='./.cache' \n",
    "logging_dir=\"./bert-base-turkish-sentiment-cased\" \n",
    "output_dir=\"./bert-base-turkish-sentiment-cased\"\n",
    "pad_to_max_length=True\n",
    "last_checkpoint=None\n",
    "resume_from_checkpoint=None\n",
    "max_train_samples=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26305160",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_list=[\"positive\",\"negative\",\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54614a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=X_train\n",
    "eval_dataset=X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_labels = datasets[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9157ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=2,\n",
    "    #finetuning_task=task_name,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"savasy/bert-base-turkish-sentiment-cased\",\n",
    "    cache_dir=cache_dir\n",
    "    #use_fast=model_args.use_fast_tokenizer\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=,\n",
    "    eval_dataset=,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "if do_train:\n",
    "    checkpoint = None\n",
    "    if resume_from_checkpoint is not None:\n",
    "        checkpoint = resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "        \n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    #max_train_samples = (\n",
    "    #    max_train_samples if max_train_samples is not None else len(train_dataset)\n",
    "    #)\n",
    "    \n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_args.pad_to_max_length:\n",
    "        padding = \"max_length\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
