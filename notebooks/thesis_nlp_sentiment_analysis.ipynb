{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef15b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cafe655",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jm/q57nwgtd485_p2qz9r5gz0y80000gn/T/ipykernel_70557/677002435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.json'"
     ]
    }
   ],
   "source": [
    "train_path = 'data/train.json'\n",
    "dev_path = 'data/validation.json'\n",
    "test_path = 'data/test.json'\n",
    "device = 'cuda'\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open(dev_path, 'r') as f:\n",
    "    dev = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dbmdz/bert-base-turkish-128k-uncased\"\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "# model_name = \"xlm-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
    "\n",
    "def to_id(text):\n",
    "    return torch.tensor(tokenizer.encode(text))\n",
    "\n",
    "def feature_extraction(data, max_seq_length=50):\n",
    "    sentences = [el[\"sentence\"] for el in data]\n",
    "    y = [mapping[el[\"value\"]] for el in data]\n",
    "    \n",
    "    return tokenizer(sentences, padding='max_length', truncation=True, max_length=max_seq_length), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOUNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0, 1, 2]\n",
    "\n",
    "x_train, y_train = feature_extraction(train)\n",
    "train_set = BOUNDataset(x_train, y_train)\n",
    "x, y = feature_extraction(dev)\n",
    "dev_set = BOUNDataset(x, y)\n",
    "x, y = feature_extraction(test)\n",
    "test_set = BOUNDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = torch.Tensor(compute_class_weight(classes=[0,1,2], y=y_train, class_weight=\"balanced\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
    "                        labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='transformer_results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=24,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=36,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='transformer_logs',\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    metric_for_best_model = \"recall\",\n",
    "    do_eval=True,\n",
    "    save_total_limit=1)\n",
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=dev_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40990e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() \n",
    "# if you want to use already finetuned model, use the command below instead\n",
    "# trainer.model = trainer.model.from_pretrained(\"berturk\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fd7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c92f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5378473",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this study, which is conducted in Turkish, \n",
    "two datasets that consist of twitter data are labeled as positive, negative, and neutral, \n",
    "while the training models with these datasets neutral tweets are ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ad5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d884963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
